{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "280902ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.1\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6cbeb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.23'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f6506eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize langchain agents and tools\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "open_api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dddfe8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting duckduckgo-search\n",
      "  Downloading duckduckgo_search-8.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting click>=8.1.8 (from duckduckgo-search)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting primp>=0.14.0 (from duckduckgo-search)\n",
      "  Using cached primp-0.14.0-cp38-abi3-win_amd64.whl.metadata (13 kB)\n",
      "Collecting lxml>=5.3.0 (from duckduckgo-search)\n",
      "  Downloading lxml-5.3.2-cp39-cp39-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: colorama in d:\\software\\python39\\lib\\site-packages (from click>=8.1.8->duckduckgo-search) (0.4.4)\n",
      "Downloading duckduckgo_search-8.0.0-py3-none-any.whl (18 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading lxml-5.3.2-cp39-cp39-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.5/3.8 MB 4.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 2.6/3.8 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 7.8 MB/s eta 0:00:00\n",
      "Using cached primp-0.14.0-cp38-abi3-win_amd64.whl (3.1 MB)\n",
      "Installing collected packages: primp, lxml, click, duckduckgo-search\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.1.7\n",
      "    Uninstalling click-8.1.7:\n",
      "      Successfully uninstalled click-8.1.7\n",
      "Successfully installed click-8.1.8 duckduckgo-search-8.0.0 lxml-5.3.2 primp-0.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing dependencies of bleach: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    tinycss2 (>=1.1.0<1.2) ; extra == 'css'\n",
      "             ~~~~~~~~^\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fe3883c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suhas\\AppData\\Local\\Temp\\ipykernel_323752\\1085996933.py:7: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(temperature =0.5)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.ddg_search.tool import DuckDuckGoSearchRun\n",
    "search_tool = Tool(\n",
    "    name= \"DuckDuckGo Search\",\n",
    "    func=DuckDuckGoSearchRun().run,\n",
    "    description=\"Use this tool to perform web searches such as latest news, entertainement movies etc using DuckDuckGo\"\n",
    ")\n",
    "llm = OpenAI(temperature =0.5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e9256c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Using cached wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\software\\python39\\lib\\site-packages (from wikipedia) (4.11.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in d:\\software\\python39\\lib\\site-packages (from wikipedia) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in d:\\software\\python39\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\software\\python39\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\software\\python39\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\software\\python39\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2022.9.24)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\software\\python39\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.3.2.post1)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (pyproject.toml): started\n",
      "  Building wheel for wikipedia (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11787 sha256=fc718dfe7db97413dc02438a0de28ffd77d2d90a4f8f68e80dc7cea2dea601e1\n",
      "  Stored in directory: c:\\users\\suhas\\appdata\\local\\pip\\cache\\wheels\\c2\\46\\f4\\caa1bee71096d7b0cdca2f2a2af45cacf35c5760bee8f00948\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing dependencies of bleach: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    tinycss2 (>=1.1.0<1.2) ; extra == 'css'\n",
      "             ~~~~~~~~^\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a28620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the Wikipedia \n",
    "from langchain_community.utilities.wikipedia import WikipediaAPIWrapper\n",
    "wikipedia_tool= Tool(\n",
    "    name= \"Wikipedia Search\",\n",
    "    func=WikipediaAPIWrapper().run,\n",
    "    description=\"Use this tool to perform wikipedia searches for information \"\n",
    ")\n",
    "\n",
    "tools=[wikipedia_tool]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4070714b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created and populated with sample data.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to SQLite database (creates example.db if it doesn't exist)\n",
    "connection = sqlite3.connect(\"example.db\")\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Create the inventory table\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS inventory (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    product_name TEXT NOT NULL,\n",
    "    stock_quantity INTEGER NOT NULL\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Insert sample data\n",
    "cursor.executemany(\"\"\"\n",
    "INSERT INTO inventory (product_name, stock_quantity) VALUES (?, ?)\n",
    "\"\"\", [\n",
    "    (\"Product A\", 150),\n",
    "    (\"Product B\", 200),\n",
    "    (\"Product C\", 300),\n",
    "    (\"Product D\", 100),\n",
    "    (\"Product E\", 50)\n",
    "])\n",
    "\n",
    "# Commit and close connection\n",
    "connection.commit()\n",
    "connection.close()\n",
    "\n",
    "print(\"Database created and populated with sample data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bb79cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///example.db\")\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "sqllite_tool = toolkit.get_tools()\n",
    "tools = sqllite_tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a5d2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a14c6042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suhas\\AppData\\Local\\Temp\\ipykernel_323752\\1771633576.py:1: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(tools = tools,\n",
      "C:\\Users\\suhas\\AppData\\Local\\Temp\\ipykernel_323752\\1771633576.py:7: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response1 = agent.run(query1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to query the database to get the number of units of Product A in stock.\n",
      "Action: sql_db_query\n",
      "Action Input: SELECT COUNT(*) FROM products WHERE name='Product A' AND stock > 0\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mError: (sqlite3.OperationalError) no such table: products\n",
      "[SQL: SELECT COUNT(*) FROM products WHERE name='Product A' AND stock > 0]\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to use sql_db_list_tables to check if the 'products' table exists.\n",
      "Action: sql_db_list_tables\n",
      "Action Input: \u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3minventory\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to use sql_db_schema to get the schema and sample rows for the 'products' table.\n",
      "Action: sql_db_schema\n",
      "Action Input: products\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mError: table_names {'products'} not found in database\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to use sql_db_query_checker to check the query before executing it.\n",
      "Action: sql_db_query_checker\n",
      "Action Input: SELECT COUNT(*) FROM inventory WHERE name='Product A' AND stock > 0\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m SELECT COUNT(*) FROM inventory WHERE name='Product A' AND stock > 0\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: SELECT COUNT(*) FROM inventory WHERE name='Product A' AND stock > 0\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "response:  SELECT COUNT(*) FROM inventory WHERE name='Product A' AND stock > 0\n"
     ]
    }
   ],
   "source": [
    "agent = initialize_agent(tools = tools, \n",
    "                         llm=llm,\n",
    "                         agent=\"zero-shot-react-description\",\n",
    "                        verbose=True\n",
    "                        )\n",
    "query1 = \"How many units of Product A are in stock?\"\n",
    "response1 = agent.run(query1)\n",
    "print(\"response: \", response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a7c1a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Downloading arxiv-2.2.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Using cached feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting requests~=2.32.0 (from arxiv)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Using cached sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\software\\python39\\lib\\site-packages (from requests~=2.32.0->arxiv) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\software\\python39\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\software\\python39\\lib\\site-packages (from requests~=2.32.0->arxiv) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\software\\python39\\lib\\site-packages (from requests~=2.32.0->arxiv) (2022.9.24)\n",
      "Downloading arxiv-2.2.0-py3-none-any.whl (11 kB)\n",
      "Using cached feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (pyproject.toml): started\n",
      "  Building wheel for sgmllib3k (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6105 sha256=2ddc93d55711e4769eb9f6b15d7cb11d67a58773d665039944d82bbe45e79880\n",
      "  Stored in directory: c:\\users\\suhas\\appdata\\local\\pip\\cache\\wheels\\65\\7a\\a7\\78c287f64e401255dff4c13fdbc672fed5efbfd21c530114e1\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, requests, feedparser, arxiv\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.1\n",
      "    Uninstalling requests-2.28.1:\n",
      "      Successfully uninstalled requests-2.28.1\n",
      "Successfully installed arxiv-2.2.0 feedparser-6.0.11 requests-2.32.3 sgmllib3k-1.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing dependencies of bleach: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    tinycss2 (>=1.1.0<1.2) ; extra == 'css'\n",
      "             ~~~~~~~~^\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ee07aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "from langchain.agents import load_tools\n",
    "arxiv_tool = Tool(\n",
    "    name=\"Arxiv Search\",\n",
    "    func= ArxivQueryRun().run,\n",
    "    description=\"Use this tool to search academic papers on arXiv. Provide topics, authors, or keywords.\"\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20587c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##tools = load_tools([\"arxiv_tool\", \"wikipedia_tool\", \"search_tool\", \"sqllite_tool\"], llm=llm)\n",
    "tools = [arxiv_tool, search_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ece4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c26ace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5d3108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, \n",
    "                         llm,\n",
    "                         agent=\"zero-shot-react-description\",\n",
    "                        verbose=True,\n",
    "                        handle_parsing_errors=True\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f4815d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ecad360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use the tools available to search for the city where Facebook office was started and recent papers on Generative AI\n",
      "Action: Arxiv Search\n",
      "Action Input: 'Facebook office location' and 'Generative AI'\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPublished: 2024-03-19\n",
      "Title: How Spammers and Scammers Leverage AI-Generated Images on Facebook for Audience Growth\n",
      "Authors: Renee DiResta, Josh A. Goldstein\n",
      "Summary: Much of the research and discourse on risks from artificial intelligence (AI)\n",
      "image generators, such as DALL-E and Midjourney, has centered around whether\n",
      "they could be used to inject false information into political discourse. We\n",
      "show that spammers and scammers - seemingly motivated by profit or clout, not\n",
      "ideology - are already using AI-generated images to gain significant traction\n",
      "on Facebook. At times, the Facebook Feed is recommending unlabeled AI-generated\n",
      "images to users who neither follow the Pages posting the images nor realize\n",
      "that the images are AI-generated, highlighting the need for improved\n",
      "transparency and provenance standards as AI models proliferate.\n",
      "\n",
      "Published: 2024-04-30\n",
      "Title: Strategic Integration of Artificial Intelligence in the C-Suite: The Role of the Chief AI Officer\n",
      "Authors: Marc Schmitt\n",
      "Summary: The integration of Artificial Intelligence (AI) into corporate strategy has\n",
      "become a pivotal focus for organizations aiming to maintain a competitive\n",
      "advantage in the digital age. As AI reshapes business operations and drives\n",
      "innovation, the need for specialized leadership to effectively manage these\n",
      "changes becomes increasingly apparent. In this paper, I explore the role of the\n",
      "Chief AI Officer (CAIO) within the C-suite, emphasizing the necessity of this\n",
      "position for successful AI strategy, integration, and governance. I analyze\n",
      "future scenarios based on current trends in three key areas: the AI Economy, AI\n",
      "Organization, and Competition in the Age of AI. These explorations lay the\n",
      "foundation for identifying the antecedents (environmental, structural, and\n",
      "strategic factors) that justify the inclusion of a CAIO in top management\n",
      "teams. This sets the stage for a comprehensive examination of the CAIO's role\n",
      "and the broader implications of AI leadership. This paper advances the\n",
      "discussion on AI leadership by providing a rationale for the strategic\n",
      "integration of AI at the executive level and examining the role of the Chief AI\n",
      "Officer within organizations.\n",
      "\n",
      "Published: 2015-06-05\n",
      "Title: Using Facebook for Image Steganography\n",
      "Authors: Jason Hiney, Tejas Dakve, Krzysztof Szczypiorski, Kris Gaj\n",
      "Summary: Because Facebook is available on hundreds of millions of desktop and mobile\n",
      "computing platforms around the world and because it is available on many\n",
      "different kinds of platforms (from desktops and laptops running Windows, Unix,\n",
      "or OS X to hand held devices running iOS, Android, or Windows Phone), it would\n",
      "seem to be the perfect place to conduct steganography. On Facebook, information\n",
      "hidden in image files will be further obscured within the millions of pictures\n",
      "and other images posted and transmitted daily. Facebook is known to alter and\n",
      "compress uploaded images so they use minimum space and bandwidth when displayed\n",
      "on Facebook pages. The compression process generally disrupts attempts to use\n",
      "Facebook for image steganography. This paper explores a method to minimize the\n",
      "disruption so JPEG images can be used as steganography carriers on Facebook.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I have found relevant papers on both topics, but I should also check if there are any news articles about the location of Facebook office.\n",
      "Action: DuckDuckGo Search\n",
      "Action Input: 'Facebook office location'\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mOther Facebook Office Locations. Facebook has a wide array of offices across the globe, each playing a significant role in the company's operations. Here are some of the most relevant locations: New York, United States: Situated at 380 W 33rd St, this office is a key player in Facebook's East Coast operations, focusing on advertising and media ... Company. Headquarters Location. Square Footage. Notable Features. Facebook: Menlo Park, CA: 430,000 sq. ft. Hangar-style engineering building, rooftop garden, fitness ... Facebook's headquarters is located at 1 Hacker Way, Menlo Park, CA 94025. The company moved to this location in 2011, after outgrowing its previous headquarters in Palo Alto, California. • Singapore: Facebook's Singapore office is at 241 Deason Road, 05-02 Tong Building, Singapore 122241. • Other locations: Facebook has offices in over 70 countries, with employees working from various locations worldwide. Conclusion. Facebook's head office in Menlo Park, California, is the hub of the company's global operations. Facebook, one of the most popular social media platforms in the world, ... This campus is located at 2164 S. Delaware St. and serves as a satellite office for Facebook's data center operations.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know that Facebook's main headquarters is located in Menlo Park, CA and they have satellite offices in various locations around the world. \n",
      "Final Answer: Facebook's main headquarters is located in Menlo Park, CA and they have satellite offices in various locations around the world. Some recent papers on Generative AI include \"How Spammers and Scammers Leverage AI-Generated Images on Facebook for Audience Growth\" by Renee DiResta and Josh A. Goldstein, \"Strategic Integration of Artificial Intelligence in the C-Suite: The Role of the Chief AI Officer\" by Marc Schmitt, and \"Using Facebook for Image Steganography\" by Jason Hiney, Tejas Dakve, Krzysztof Szczypiorski, and Kris Gaj.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "response:  Facebook's main headquarters is located in Menlo Park, CA and they have satellite offices in various locations around the world. Some recent papers on Generative AI include \"How Spammers and Scammers Leverage AI-Generated Images on Facebook for Audience Growth\" by Renee DiResta and Josh A. Goldstein, \"Strategic Integration of Artificial Intelligence in the C-Suite: The Role of the Chief AI Officer\" by Marc Schmitt, and \"Using Facebook for Image Steganography\" by Jason Hiney, Tejas Dakve, Krzysztof Szczypiorski, and Kris Gaj.\n"
     ]
    }
   ],
   "source": [
    "##query1 = \"Search for research papers on Diffusion Models in 2024.\"\n",
    "##query1 = \"Find recent papers on Generative AI and what is the latest movie trending in Charlotte USA this week\"\n",
    "query1 = \"In which city was Facebook office started and What are some of the recent papers on Generative AI\"\n",
    "response1 = agent.run(query1)\n",
    "print(\"response: \", response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76f5b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvWFDemo",
   "language": "python",
   "name": "venvwfdemo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
