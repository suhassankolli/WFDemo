{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "280902ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.0\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6cbeb7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.20'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "langchain.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f6506eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize langchain agents and tools\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "open_api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dddfe8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting duckduckgo-search\n",
      "  Downloading duckduckgo_search-2025.4.4-cp310-cp310-win_amd64.whl.metadata (18 kB)\n",
      "Requirement already satisfied: click>=8.1.8 in d:\\learning\\gen ai\\wfdemo\\venvwfdemo\\lib\\site-packages (from duckduckgo-search) (8.1.8)\n",
      "Collecting primp>=0.14.0 (from duckduckgo-search)\n",
      "  Downloading primp-0.14.0-cp38-abi3-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: lxml>=5.3.0 in d:\\learning\\gen ai\\wfdemo\\venvwfdemo\\lib\\site-packages (from duckduckgo-search) (5.3.1)\n",
      "Requirement already satisfied: colorama in d:\\learning\\gen ai\\wfdemo\\venvwfdemo\\lib\\site-packages (from click>=8.1.8->duckduckgo-search) (0.4.6)\n",
      "Downloading duckduckgo_search-2025.4.4-cp310-cp310-win_amd64.whl (60 kB)\n",
      "Downloading primp-0.14.0-cp38-abi3-win_amd64.whl (3.1 MB)\n",
      "   ---------------------------------------- 0.0/3.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.1/3.1 MB 16.7 MB/s eta 0:00:00\n",
      "Installing collected packages: primp, duckduckgo-search\n",
      "Successfully installed duckduckgo-search-2025.4.4 primp-0.14.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U duckduckgo-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4fe3883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.ddg_search.tool import DuckDuckGoSearchRun\n",
    "search_tool = Tool(\n",
    "    name= \"DuckDuckGo Search\",\n",
    "    func=DuckDuckGoSearchRun().run,\n",
    "    description=\"Use this tool to perform web searches such as latest news, entertainement movies etc using DuckDuckGo\"\n",
    ")\n",
    "llm = OpenAI(temperature =0.5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e9256c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wikipedia\n",
      "  Using cached wikipedia-1.4.0.tar.gz (27 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\learning\\gen ai\\wfdemo\\venvwfdemo\\lib\\site-packages (from wikipedia) (4.13.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.0.0 in d:\\learning\\gen ai\\wfdemo\\venvwfdemo\\lib\\site-packages (from wikipedia) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\learning\\gen ai\\wfdemo\\venvwfdemo\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\learning\\gen ai\\wfdemo\\venvwfdemo\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\learning\\gen ai\\wfdemo\\venvwfdemo\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\learning\\gen ai\\wfdemo\\venvwfdemo\\lib\\site-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.1.31)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\learning\\gen ai\\wfdemo\\venvwfdemo\\lib\\site-packages (from beautifulsoup4->wikipedia) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\learning\\gen ai\\wfdemo\\venvwfdemo\\lib\\site-packages (from beautifulsoup4->wikipedia) (4.12.2)\n",
      "Building wheels for collected packages: wikipedia\n",
      "  Building wheel for wikipedia (setup.py): started\n",
      "  Building wheel for wikipedia (setup.py): finished with status 'done'\n",
      "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11756 sha256=b4794d00a4f499b9336a493655db2be5b5a1e7bfd2f4d2f00cafb80ae3a08bfe\n",
      "  Stored in directory: c:\\users\\suhas\\appdata\\local\\pip\\cache\\wheels\\5e\\b6\\c5\\93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
      "Successfully built wikipedia\n",
      "Installing collected packages: wikipedia\n",
      "Successfully installed wikipedia-1.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a28620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the Wikipedia \n",
    "from langchain_community.utilities.wikipedia import WikipediaAPIWrapper\n",
    "wikipedia_tool= Tool(\n",
    "    name= \"Wikipedia Search\",\n",
    "    func=WikipediaAPIWrapper().run,\n",
    "    description=\"Use this tool to perform wikipedia searches for information \"\n",
    ")\n",
    "\n",
    "tools=[wikipedia_tool]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4070714b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database created and populated with sample data.\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Connect to SQLite database (creates example.db if it doesn't exist)\n",
    "connection = sqlite3.connect(\"example.db\")\n",
    "cursor = connection.cursor()\n",
    "\n",
    "# Create the inventory table\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS inventory (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    product_name TEXT NOT NULL,\n",
    "    stock_quantity INTEGER NOT NULL\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Insert sample data\n",
    "cursor.executemany(\"\"\"\n",
    "INSERT INTO inventory (product_name, stock_quantity) VALUES (?, ?)\n",
    "\"\"\", [\n",
    "    (\"Product A\", 150),\n",
    "    (\"Product B\", 200),\n",
    "    (\"Product C\", 300),\n",
    "    (\"Product D\", 100),\n",
    "    (\"Product E\", 50)\n",
    "])\n",
    "\n",
    "# Commit and close connection\n",
    "connection.commit()\n",
    "connection.close()\n",
    "\n",
    "print(\"Database created and populated with sample data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bb79cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities.sql_database import SQLDatabase\n",
    "from langchain_community.agent_toolkits.sql.toolkit import SQLDatabaseToolkit\n",
    "\n",
    "db = SQLDatabase.from_uri(\"sqlite:///example.db\")\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
    "sqllite_tool = toolkit.get_tools()\n",
    "tools = sqllite_tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a5d2dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a14c6042",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suhas\\AppData\\Local\\Temp\\ipykernel_42060\\1771633576.py:1: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(tools = tools,\n",
      "C:\\Users\\suhas\\AppData\\Local\\Temp\\ipykernel_42060\\1771633576.py:7: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response1 = agent.run(query1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m We need to find the table that contains information about Product A and its stock levels.\n",
      "Action: sql_db_list_tables\n",
      "Action Input: \"\"\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3minventory\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Now that we know the table, we can use sql_db_schema to see its fields and sample rows.\n",
      "Action: sql_db_schema\n",
      "Action Input: inventory\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3m\n",
      "CREATE TABLE inventory (\n",
      "\tid INTEGER, \n",
      "\tproduct_name TEXT NOT NULL, \n",
      "\tstock_quantity INTEGER NOT NULL, \n",
      "\tPRIMARY KEY (id)\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from inventory table:\n",
      "id\tproduct_name\tstock_quantity\n",
      "1\tProduct A\t150\n",
      "2\tProduct B\t200\n",
      "3\tProduct C\t300\n",
      "*/\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m We need to use a SQL query to retrieve the stock quantity for Product A.\n",
      "Action: sql_db_query\n",
      "Action Input: SELECT stock_quantity FROM inventory WHERE product_name = 'Product A'\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[(150,)]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m We can use sql_db_query_checker to double check our query before executing it.\n",
      "Action: sql_db_query_checker\n",
      "Action Input: SELECT stock_quantity FROM inventory WHERE product_name = 'Product A'\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m\n",
      "\n",
      "SELECT stock_quantity FROM inventory WHERE product_name = 'Product A'\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m The query is correct, so we can now execute it with sql_db_query.\n",
      "Action: sql_db_query\n",
      "Action Input: SELECT stock_quantity FROM inventory WHERE product_name = 'Product A'\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[(150,)]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: There are 150 units of Product A in stock.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "response:  There are 150 units of Product A in stock.\n"
     ]
    }
   ],
   "source": [
    "agent = initialize_agent(tools = tools, \n",
    "                         llm=llm,\n",
    "                         agent=\"zero-shot-react-description\",\n",
    "                        verbose=True\n",
    "                        )\n",
    "query1 = \"How many units of Product A are in stock?\"\n",
    "response1 = agent.run(query1)\n",
    "print(\"response: \", response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a7c1a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting arxiv\n",
      "  Using cached arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting feedparser~=6.0.10 (from arxiv)\n",
      "  Using cached feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: requests~=2.32.0 in d:\\learning\\gen ai\\wfdemo\\venvwfdemo\\lib\\site-packages (from arxiv) (2.32.3)\n",
      "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
      "  Using cached sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\learning\\gen ai\\wfdemo\\venvwfdemo\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\learning\\gen ai\\wfdemo\\venvwfdemo\\lib\\site-packages (from requests~=2.32.0->arxiv) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\learning\\gen ai\\wfdemo\\venvwfdemo\\lib\\site-packages (from requests~=2.32.0->arxiv) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\learning\\gen ai\\wfdemo\\venvwfdemo\\lib\\site-packages (from requests~=2.32.0->arxiv) (2025.1.31)\n",
      "Using cached arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
      "Using cached feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6104 sha256=6377da0424519817053b85f25034a001c0312d3cfe9e55cd4c678dde1b5a610a\n",
      "  Stored in directory: c:\\users\\suhas\\appdata\\local\\pip\\cache\\wheels\\f0\\69\\93\\a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
      "Successfully installed arxiv-2.1.3 feedparser-6.0.11 sgmllib3k-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ee07aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
    "from langchain.agents import load_tools\n",
    "arxiv_tool = Tool(\n",
    "    name=\"Arxiv Search\",\n",
    "    func= ArxivQueryRun().run,\n",
    "    description=\"Use this tool to search academic papers on arXiv. Provide topics, authors, or keywords.\"\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20587c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##tools = load_tools([\"arxiv_tool\", \"wikipedia_tool\", \"search_tool\", \"sqllite_tool\"], llm=llm)\n",
    "tools = [arxiv_tool, search_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ece4ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c26ace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e5d3108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, \n",
    "                         llm,\n",
    "                         agent=\"zero-shot-react-description\",\n",
    "                        verbose=True,\n",
    "                        handle_parsing_errors=True\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f4815d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ecad360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should use the Arxiv Search tool to find recent papers on Generative AI and use the DuckDuckGo Search tool to find information about the location of the Facebook office.\n",
      "Action: Arxiv Search\n",
      "Action Input: \"Generative AI\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPublished: 2025-02-16\n",
      "Title: AI Generations: From AI 1.0 to AI 4.0\n",
      "Authors: Jiahao Wu, Hengxu You, Jing Du\n",
      "Summary: This paper proposes that Artificial Intelligence (AI) progresses through\n",
      "several overlapping generations: AI 1.0 (Information AI), AI 2.0 (Agentic AI),\n",
      "AI 3.0 (Physical AI), and now a speculative AI 4.0 (Conscious AI). Each of\n",
      "these AI generations is driven by shifting priorities among algorithms,\n",
      "computing power, and data. AI 1.0 ushered in breakthroughs in pattern\n",
      "recognition and information processing, fueling advances in computer vision,\n",
      "natural language processing, and recommendation systems. AI 2.0 built on these\n",
      "foundations through real-time decision-making in digital environments,\n",
      "leveraging reinforcement learning and adaptive planning for agentic AI\n",
      "applications. AI 3.0 extended intelligence into physical contexts, integrating\n",
      "robotics, autonomous vehicles, and sensor-fused control systems to act in\n",
      "uncertain real-world settings. Building on these developments, AI 4.0 puts\n",
      "forward the bold vision of self-directed AI capable of setting its own goals,\n",
      "orchestrating complex training regimens, and possibly exhibiting elements of\n",
      "machine consciousness. This paper traces the historical foundations of AI\n",
      "across roughly seventy years, mapping how changes in technological bottlenecks\n",
      "from algorithmic innovation to high-performance computing to specialized data,\n",
      "have spurred each generational leap. It further highlights the ongoing\n",
      "synergies among AI 1.0, 2.0, 3.0, and 4.0, and explores the profound ethical,\n",
      "regulatory, and philosophical challenges that arise when artificial systems\n",
      "approach (or aspire to) human-like autonomy. Ultimately, understanding these\n",
      "evolutions and their interdependencies is pivotal for guiding future research,\n",
      "crafting responsible governance, and ensuring that AI transformative potential\n",
      "benefits society as a whole.\n",
      "\n",
      "Published: 2021-03-29\n",
      "Title: \"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest Value for us?\n",
      "Authors: Bin Liu\n",
      "Summary: AI has surpassed humans across a variety of tasks such as image\n",
      "classification, playing games (e.g., go, \"Starcraft\" and poker), and protein\n",
      "structure prediction. However, at the same time, AI is also bearing serious\n",
      "controversies. Many researchers argue that little substantial progress has been\n",
      "made for AI in recent decades. In this paper, the author (1) explains why\n",
      "controversies about AI exist; (2) discriminates two paradigms of AI research,\n",
      "termed \"weak AI\" and \"strong AI\" (a.k.a. artificial general intelligence); (3)\n",
      "clarifies how to judge which paradigm a research work should be classified\n",
      "into; (4) discusses what is the greatest value of \"weak AI\" if it has no chance\n",
      "to develop into \"strong AI\".\n",
      "\n",
      "Published: 2024-02-08\n",
      "Title: A Bibliometric View of AI Ethics Development\n",
      "Authors: Di Kevin Gao, Andrew Haverly, Sudip Mittal, Jingdao Chen\n",
      "Summary: Artificial Intelligence (AI) Ethics is a nascent yet critical research field.\n",
      "Recent developments in generative AI and foundational models necessitate a\n",
      "renewed look at the problem of AI Ethics. In this study, we perform a\n",
      "bibliometric analysis of AI Ethics literature for the last 20 years based on\n",
      "keyword search. Our study reveals a three-phase development in AI Ethics,\n",
      "namely an incubation phase, making AI human-like machines phase, and making AI\n",
      "human-centric machines phase. We conjecture that the next phase of AI ethics is\n",
      "likely to focus on making AI more machine-like as AI matches or surpasses\n",
      "humans intellectually, a term we coin as \"machine-like human\".\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should now use the DuckDuckGo Search tool to find information about the location of the Facebook office.\n",
      "Action: DuckDuckGo Search\n",
      "Action Input: \"Facebook office location\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mCompany. Headquarters Location. Square Footage. Notable Features. Facebook: Menlo Park, CA: 430,000 sq. ft. Hangar-style engineering building, rooftop garden, fitness ... Other Facebook Office Locations. Facebook has a wide array of offices across the globe, each playing a significant role in the company's operations. Here are some of the most relevant locations: New York, United States: Situated at 380 W 33rd St, this office is a key player in Facebook's East Coast operations, focusing on advertising and media ... Facebook, one of the most popular social media platforms in the world, ... This campus is located at 2164 S. Delaware St. and serves as a satellite office for Facebook's data center operations. Facebook's headquarters is located at 1 Hacker Way, Menlo Park, CA 94025. The company moved to this location in 2011, after outgrowing its previous headquarters in Palo Alto, California. • Singapore: Facebook's Singapore office is at 241 Deason Road, 05-02 Tong Building, Singapore 122241. • Other locations: Facebook has offices in over 70 countries, with employees working from various locations worldwide. Conclusion. Facebook's head office in Menlo Park, California, is the hub of the company's global operations.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: Facebook's office was started in Menlo Park, California and some recent papers on Generative AI include \"AI Generations: From AI 1.0 to AI 4.0\" by Jiahao Wu, Hengxu You, and Jing Du, \"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest Value for us?\" by Bin Liu, and \"A Bibliometric View of AI Ethics Development\" by Di Kevin Gao, Andrew Haverly, Sudip Mittal, and Jingdao Chen.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "response:  Facebook's office was started in Menlo Park, California and some recent papers on Generative AI include \"AI Generations: From AI 1.0 to AI 4.0\" by Jiahao Wu, Hengxu You, and Jing Du, \"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest Value for us?\" by Bin Liu, and \"A Bibliometric View of AI Ethics Development\" by Di Kevin Gao, Andrew Haverly, Sudip Mittal, and Jingdao Chen.\n"
     ]
    }
   ],
   "source": [
    "##query1 = \"Search for research papers on Diffusion Models in 2024.\"\n",
    "##query1 = \"Find recent papers on Generative AI and what is the latest movie trending in Charlotte USA this week\"\n",
    "query1 = \"In which city was Facebook office started and What are some of the recent papers on Generative AI\"\n",
    "response1 = agent.run(query1)\n",
    "print(\"response: \", response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76f5b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvWFDemo",
   "language": "python",
   "name": "venvwfdemo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
